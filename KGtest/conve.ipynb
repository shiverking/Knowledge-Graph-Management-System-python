{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三元组数： 181\n"
     ]
    }
   ],
   "source": [
    "#导入数据集\n",
    "import pandas as pd\n",
    "df = pd.read_excel('/home/cjw/KGtest/triples.xls')\n",
    "row_num = len(df.index.values)\n",
    "print(\"三元组数：\", row_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 6:2:2划分为训练集、验证集、测试集\n",
    "train, valid, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "writer = pd.ExcelWriter('/home/cjw/KGtest/train.xlsx')\n",
    "train.to_excel(writer, 'Sheet', index=False)\n",
    "writer.save()\n",
    "writer = pd.ExcelWriter('/home/cjw/KGtest/valid.xlsx')\n",
    "valid.to_excel(writer, 'Sheet', index=False)\n",
    "writer.save()\n",
    "writer = pd.ExcelWriter('/home/cjw/KGtest/test.xlsx')\n",
    "test.to_excel(writer, 'Sheet', index=False)\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ordered_set import OrderedSet\n",
    "from collections import defaultdict as ddict, Counter\n",
    "from torch.utils.data import DataLoader\n",
    "# 数据预处理\n",
    "def load_data():\n",
    "    ent_set, rel_set = OrderedSet(), OrderedSet()\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for line in pd.read_excel(f'/home/cjw/KGtest/{split}.xlsx').values.tolist():\n",
    "            sub, rel, obj = line[0], line[4], line[2]\n",
    "            ent_set.add(sub)\n",
    "            rel_set.add(rel)\n",
    "            ent_set.add(obj)\n",
    "\n",
    "    ent2id = {ent: idx for idx, ent in enumerate(ent_set)}\n",
    "    rel2id = {rel: idx for idx, rel in enumerate(rel_set)}\n",
    "    rel2id.update({rel + '_reverse': idx + len(rel2id) for idx, rel in enumerate(rel_set)})\n",
    "    \n",
    "    id2ent = {idx: ent for ent, idx in ent2id.items()}\n",
    "    id2rel = {idx: rel for rel, idx in rel2id.items()}\n",
    "\n",
    "    num_ent = len(ent2id)\n",
    "    num_rel = len(rel2id) // 2\n",
    "    # embed_dim = self.p.k_w * self.p.k_h if self.p.embed_dim is None else self.p.embed_dim\n",
    "\n",
    "    data = ddict(list)\n",
    "    sr2o = ddict(set)\n",
    "\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for line in pd.read_excel(f'/home/cjw/KGtest/{split}.xlsx').values.tolist():\n",
    "            sub, rel, obj = line[0], line[4], line[2]\n",
    "            sub, rel, obj = ent2id[sub], rel2id[rel], ent2id[obj]\n",
    "            data[split].append((sub, rel, obj))\n",
    "            if split == 'train':\n",
    "                sr2o[(sub, rel)].add(obj)\n",
    "                sr2o[(obj, rel + num_rel)].add(sub)\n",
    "    data = dict(data)\n",
    "    sr2o = {k: list(v) for k, v in sr2o.items()}\n",
    "    # for split in ['test', 'valid']:\n",
    "    #     for sub, rel, obj in data[split]:\n",
    "    #         sr2o[(sub, rel)].add(obj)\n",
    "    #         sr2o[(obj, rel + num_rel)].add(sub)\n",
    "\n",
    "    sr2o_all = {k: list(v) for k, v in sr2o.items()}\n",
    "    triples = ddict(list)\n",
    "    for (sub, rel), obj in sr2o.items():\n",
    "        triples['train'].append({'triple': (sub, rel, -1), 'label': sr2o[(sub, rel)], 'sub_samp': 1})\n",
    "        \n",
    "    # for split in ['test', 'valid']:\n",
    "    #     for sub, rel, obj in data[split]:\n",
    "    #         rel_inv = rel + num_rel\n",
    "    #         triples['{}_{}'.format(split, 'tail')].append({'triple': (sub, rel, obj), 'label': sr2o_all[(sub, rel)]})\n",
    "    #         triples['{}_{}'.format(split, 'head')].append({'triple': (obj, rel_inv, sub), 'label': sr2o_all[(obj, rel_inv)]})\n",
    "    triples = dict(triples)\n",
    "    print(triples)\n",
    "\n",
    "    num_workers = 0\n",
    "    def get_data_loader(dataset_class, split, batch_size, shuffle=True):\n",
    "        return DataLoader(\n",
    "            dataset_class(triples[split])\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=max(0, num_workers),\n",
    "            collate_fn=dataset_class.collate_fn\n",
    "        )\n",
    "\n",
    "    self.data_iter = {\n",
    "        'train'\t:   get_data_loader(TrainDataset, 'train', \tself.p.batch_size),\n",
    "        'valid_head'\t:   get_data_loader(TestDataset,  'valid_head', self.p.batch_size),\n",
    "        'valid_tail'\t:   get_data_loader(TestDataset,  'valid_tail', self.p.batch_size),\n",
    "        'test_head'\t:   get_data_loader(TestDataset,  'test_head',  self.p.batch_size),\n",
    "        'test_tail'\t:   get_data_loader(TestDataset,  'test_tail',  self.p.batch_size),\n",
    "    }\n",
    "\n",
    "\n",
    "        \n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "\tdef __init__(self, triples, params):\n",
    "\t\tself.triples\t= triples\n",
    "\t\tself.p \t\t= params\n",
    "\t\tself.entities\t= np.arange(self.p.num_ent, dtype=np.int32)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.triples)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tele\t\t\t= self.triples[idx]\n",
    "\t\ttriple, label, sub_samp\t= torch.LongTensor(ele['triple']), np.int32(ele['label']), np.float32(ele['sub_samp'])\n",
    "\t\ttrp_label\t\t= self.get_label(label)\n",
    "\n",
    "\t\tif self.p.lbl_smooth != 0.0:\n",
    "\t\t\ttrp_label = (1.0 - self.p.lbl_smooth)*trp_label + (1.0/self.p.num_ent)\n",
    "\n",
    "\t\treturn triple, trp_label, None, None\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef collate_fn(data):\n",
    "\t\ttriple\t\t= torch.stack([_[0] \tfor _ in data], dim=0)\n",
    "\t\ttrp_label\t= torch.stack([_[1] \tfor _ in data], dim=0)\n",
    "\n",
    "\t\treturn triple, trp_label\n",
    "\t\n",
    "\tdef get_label(self, label):\n",
    "\t\ty = np.zeros([self.p.num_ent], dtype=np.float32)\n",
    "\t\tfor e2 in label: y[e2] = 1.0\n",
    "\t\treturn torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "\tdef __init__(self, triples, params):\n",
    "\t\tself.triples\t= triples\n",
    "\t\tself.p \t\t= params\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.triples)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tele\t\t= self.triples[idx]\n",
    "\t\ttriple, label\t= torch.LongTensor(ele['triple']), np.int32(ele['label'])\n",
    "\t\tlabel\t\t= self.get_label(label)\n",
    "\n",
    "\t\treturn triple, label\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef collate_fn(data):\n",
    "\t\ttriple\t\t= torch.stack([_[0] \tfor _ in data], dim=0)\n",
    "\t\tlabel\t\t= torch.stack([_[1] \tfor _ in data], dim=0)\n",
    "\t\treturn triple, label\n",
    "\t\n",
    "\tdef get_label(self, label):\n",
    "\t\ty = np.zeros([self.p.num_ent], dtype=np.float32)\n",
    "\t\tfor e2 in label: y[e2] = 1.0\n",
    "\t\treturn torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self):\n",
    "\n",
    "        ent_set, rel_set = OrderedSet(), OrderedSet()\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            for line in open('../data/{}/{}.txt'.format(self.p.dataset, split)):\n",
    "                sub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
    "                ent_set.add(sub)\n",
    "                rel_set.add(rel)\n",
    "                ent_set.add(obj)\n",
    "\n",
    "        self.ent2id = {ent: idx for idx, ent in enumerate(ent_set)}\n",
    "        self.rel2id = {rel: idx for idx, rel in enumerate(rel_set)}\n",
    "        self.rel2id.update({rel + '_reverse': idx + len(self.rel2id) for idx, rel in enumerate(rel_set)})\n",
    "        \n",
    "\n",
    "        self.id2ent = {idx: ent for ent, idx in self.ent2id.items()}\n",
    "        self.id2rel = {idx: rel for rel, idx in self.rel2id.items()}\n",
    "\n",
    "        self.p.num_ent = len(self.ent2id)\n",
    "        self.p.num_rel = len(self.rel2id) // 2\n",
    "        self.p.embed_dim = self.p.k_w * self.p.k_h if self.p.embed_dim is None else self.p.embed_dim\n",
    "\n",
    "        self.data = ddict(list)\n",
    "        sr2o = ddict(set)\n",
    "\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            for line in open('../data/{}/{}.txt'.format(self.p.dataset, split)):\n",
    "                sub, rel, obj = map(str.lower, line.strip().split('\\t'))\n",
    "                sub, rel, obj = self.ent2id[sub], self.rel2id[rel], self.ent2id[obj]\n",
    "\n",
    "                self.data[split].append((sub, rel, obj))\n",
    "\n",
    "                if split == 'train':\n",
    "                    sr2o[(sub, rel)].add(obj)\n",
    "                    sr2o[(obj, rel + self.p.num_rel)].add(sub)\n",
    "        self.data = dict(self.data)\n",
    "\n",
    "        self.sr2o = {k: list(v) for k, v in sr2o.items()}\n",
    "        for split in ['test', 'valid']:\n",
    "            for sub, rel, obj in self.data[split]:\n",
    "                sr2o[(sub, rel)].add(obj)\n",
    "                sr2o[(obj, rel + self.p.num_rel)].add(sub)\n",
    "\n",
    "        self.sr2o_all = {k: list(v) for k, v in sr2o.items()}\n",
    "        self.triples = ddict(list)\n",
    "\n",
    "        for (sub, rel), obj in self.sr2o.items():\n",
    "            self.triples['train'].append({'triple': (sub, rel, -1), 'label': self.sr2o[(sub, rel)], 'sub_samp': 1})\n",
    "\n",
    "\n",
    "        for split in ['test', 'valid']:\n",
    "            for sub, rel, obj in self.data[split]:\n",
    "                rel_inv = rel + self.p.num_rel\n",
    "                self.triples['{}_{}'.format(split, 'tail')].append(\n",
    "                    {'triple': (sub, rel, obj), 'label': self.sr2o_all[(sub, rel)]})\n",
    "                self.triples['{}_{}'.format(split, 'head')].append(\n",
    "                    {'triple': (obj, rel_inv, sub), 'label': self.sr2o_all[(obj, rel_inv)]})\n",
    "        print(self.triples)\n",
    "\n",
    "        self.triples = dict(self.triples)\n",
    "\n",
    "        def get_data_loader(dataset_class, split, batch_size, shuffle=True):\n",
    "            return DataLoader(\n",
    "                dataset_class(self.triples[split], self.p),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                num_workers=max(0, self.p.num_workers),\n",
    "                collate_fn=dataset_class.collate_fn\n",
    "            )\n",
    "\n",
    "        self.data_iter = {\n",
    "            'train'\t:   get_data_loader(TrainDataset, 'train', \tself.p.batch_size),\n",
    "            'valid_head'\t:   get_data_loader(TestDataset,  'valid_head', self.p.batch_size),\n",
    "            'valid_tail'\t:   get_data_loader(TestDataset,  'valid_tail', self.p.batch_size),\n",
    "            'test_head'\t:   get_data_loader(TestDataset,  'test_head',  self.p.batch_size),\n",
    "            'test_tail'\t:   get_data_loader(TestDataset,  'test_tail',  self.p.batch_size),\n",
    "        }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deepke': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1591937cf4d2926c24f2e71b9c5232040bb2cdfc469192012c27986de8774182"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
